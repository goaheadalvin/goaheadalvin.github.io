---
layout: post
title: "神经网络简介"
excerpt: "人工智能，神经网络，机器学习，深度学习都是什么嘛"
categories: blog
tags: [编程, 神经网络]
author: Alvin
comments: true
share: true
---
{% include _toc.html %}  



## ANN（Artificial Neural Network） 

>让我们先来看看那些容易和神经网络混淆的概念
<figure >
<img src="/postimage/AI1/神经网络.jpg" alt="wechat">
</figure>   

 
以数学和物理方法以及信息处理的角度对人脑神经网络进行抽象，并建立简化的模型，称为人工神经网络，简称为神经网络.   

  
那么，怎么用数学和物理方法等来进行抽象，即神经网络怎么学习呢。    

<a href="https://www.quora.com/How-do-artificial-neural-networks-learn" target="_blank">How do artificial neural networks learn?</a>   

>Thus "learning" in a neural network  is nothing but iterating between computing the error using the training data and updating the weights by calculating the gradient of the error function.    
 
>We start by computing the gradients from the top layer, and store the gradients as we progress down each layer,so that we need not recompute the gradients. This leads to computational efficiency.    
    
The learning mechanisms of a neural network   
  
>Looking at an analogy may be useful in understanding the mechanisms of a neural network. Learning in a neural network is closely related to how we learn in our regular lives and activities - we perform an action and are either accepted or corrected by a trainer or coach to understand how to get better at a certain task. Similarly, neural networks require a trainer in order to describe what should have been produced as a response to the input. Based on the difference between the actual value and the value that was outputted by the network, an error value is computed and sent back through the system. For each layer of the network, the error value is analyzed and used to adjust the threshold and weights for the next input. In this way, the error keeps becoming marginally lesser each run as the network learns how to analyze values.  

>The procedure described above is known as backpropogation, and is applied continuously through a network until the error value is kept at a minimum. At this point, the neural network no longer requires such a training process and is allowed to run without adjustments. The network may then finally be applied, using the adjusted weights and thresholds as guidelines. 

The usage of a neural network while running   

>When a neural network is actively running, no backpropogation takes place as there is no way to directly verify the expected response. Instead, the validity of output statements are corrected during a new training session or are left as is for the network to run. Many adjustments may need to be made as the network consists of a great amount of variables that must be precise for the artificial neural network to function.  

>A basic example of such a process can be examined by teaching a neural network to convert text to speech. One could pick multiple different articles and paragraphs and use them as inputs for the network and predetermine a desired input before running the test. The training phase would then consist of going through the multiple layers of the network and using backpropogation to adjust the parameters and threshold value of the network in order to minimize the error value for all input examples. The network may then be tested on new articles to determine if it could truly convert text to proper speech.

>Networks like these may be viable models for a great array of mathematical and statistical problems, including but not limited to speech synthesis and recognition, face recognition and prediction, nonlinear system modeling and pattern classification. 

Conclusion   

>Neural networks are a new concept whose potential we have just scratched the surface of. They may be used for a variety of different concepts and ideas, and learn through a specific mechanism of backpropogation and error correction during the testing phase. By properly minimizing the error, these multi-layered systems may be able to one day learn and conceptualize ideas alone, without human correction. 

     
通俗点讲可以是:  

>你需要挑选芒果，你不知道什么样的芒果最好吃，所以你就尝遍了所有的芒果，然后自己总结出个大深黄色的比较好吃，以后再去买的时候，就可以直接挑选这种。
那什么是机器学习呢，就是你让机器“尝”一遍所有芒果，当然，也假设它知道哪些好吃，让机器去总结一套规律（个大深黄色），这就是机器学习。具体操作，就是你描述给机器每一个芒果的特征（颜色，大小，软硬……），描述给机器其输出（味道如何，是否好吃），剩下的就等机器去学习出一套规则。  
等等，那机器是怎么学习到这个规则（个大深黄色的好吃）的？没错，是通过机器学习算法。而题主所问的神经网络，恰好就是一种机器学习算法！近些年来，由于深度学习概念的兴起，神经网络又成为了机器学习领域最热门的研究方法。  

>神经网络就像一个刚开始学习东西的小孩子，开始认东西，作为一个大人（监督者），第一天，他看见一只京巴狗，你告诉他这是狗；第二天他看见一只波斯猫，他开心地说，这是狗，纠正他，这是猫；第三天，他看见一只蝴蝶犬，他又迷惑了，你告诉他这是狗……直到有一天，他可以分清任何一只猫或者狗。  
其实神经网络最初得名，就是其在模拟人的大脑，把每一个节点当作一个神经元，这些“神经元”组成的网络就是神经网络。而由于计算机出色的计算能力和细节把握能力，在大数据的基础上，神经网络往往有比人更出色的表现。  
当然了，也可以把神经网络当作一个黑箱子，只要告诉它输入，输出，他可以学到输入与输出的函数关系。神经网络的理论基础之一是三层的神经网络可以逼近任意的函数，所以理论上，只要数据量够大，“箱子容量”够大（神经元数量），神经网络就可以学到你要的东西。  

>神经网络可以看成“非线性方程组基于随机概率的暴力近似求解”。


那么，神经网络主要用来做些什么呢  

>神经网络的重要用途之一是分类，为了让大家对分类有个直观的认识，咱们先看几个例子：  
垃圾邮件识别：现在有一封电子邮件，把出现在里面的所有词汇提取出来，送进一个机器里，机器需要判断这封邮件是否是垃圾邮件。  
疾病判断：病人到医院去做了一大堆肝功、尿检测验，把测验结果送进一个机器里，机器需要判断这个病人是否得病，得的什么病。  
猫狗分类：有一大堆猫、狗照片，把每一张照片送进一个机器里，机器需要判断这幅照片里的东西是猫还是狗。  
这种能自动对输入的东西进行分类的机器，就叫做分类器。  

---

## 前导知识  

### &nbsp; &nbsp;入门
  
1. 线性代数（初级）：神经网络里涉及到最多的就是矩阵运算了
2. 微积分（初级）：反向传播（back propagation）就是求导链式法则的应用  
3. 线性代数+微积分：学会对矩阵函数（变量是矩阵的函数）求导之后，计算会快很多  
4. 编程语言（matlab/python优先）：这两种语言可以很方便做线性代数计算（python可以用numpy）


#### 理解 

5. 线性代数（中级）：特征值特征向量、线性方程组等等，为优化做准备
6. 微积分（中级）：梯度、Hessian、曲率等等，为优化做准备
7. 凸优化（中级）：优化算法等，结合线性代数和微积分的知识，可以更好地了解怎么训练神经网络
8. 概率统计/随机数学（中级）：了解一些深层神经网络的概率模型，比如resticted Boltzmann machines之类的

####应用  

9. 计算机视觉/自然语言处理/语音识别等（中级）：应用背景了解
10. 计算机原理/体系结构（中级）：了解存储器层级（memory hierarchy）、I/O等等
11. 编译原理（中级）：了解代码优化
12. 并行计算（中级/高级）：了解GPU计算、多线程计算、分布式计算等等

#### 理论研究

13. 凸优化（高级）：优化算法背后的理论（online learning等等）、近几年的paper
14. 非凸优化（中级/高级）：近几年的paper，可能对神经网络优化有借鉴/指导意义
15. 概率统计/随机数学（高级）：可能对深入了解以概率图模型（probabilistic graphical models）建模神经网络的方法有帮助
16. 其他可能尚未成体系的模型、优化算法、目标函数等等：参见近几年的paper    

---

## 历史阶段
- 人工神经元的数理模型（即神经元的阀值模型，简称 MP 模型）
- 连接权值强化的Hebb法则   

>神经元之间突触的联系强度是可变的，这种可变性是学习和记忆
的基础。  

- 长枪乌贼巨大轴索非线性动力学微分方程，即 H-H 方程。  

>这一方程可用来描述神经膜中所发生的非线性现象如自激震荡、混沌及多重稳定性等问题,所以有重大的理论与应用价值。  

- MP模型的基础上增加了学习机制--感知器模型  

>证明了两层感知器能够对输入进行分类，他还提出了带隐层处理元件的三层感知器这一重要的研究方向.  

- ADALINE网络模型  

>是一种连续取值的自适应线性神经元网络模型，可以用于自适应系统。他们针对输入为线性可分的问题进行了研究，得出期望响应与计算响应的误差可能搜索到全局最小值.  

- 在神经元模型中引入了不应期特性  

> 研究神经网络中会出现的回响现象  
  
- Hopfield 模型  

>Hopfield 的模型不仅对人工神经网络信息存储和提取功能进行了非线性
数学概括，提出了动力方程和学习方程，还对网络算法提供了重要公式和参数，使人工神经网络的构造和学习有了理论指导.  

- 模拟退火算法  

>NP 完全组合优化问题的求解，这种模拟高温物体退火过程来找寻全
局最优解的方法最早由 Metropli 等人 1953 年提出的。  

- Boltzmann 机  

>大规模并行网络学习机，并明确提出隐单元的概念  

- 并行分布处理理论  

> 主要致力于认知的微观研究，同时对具有非线性连续转移函数的多层前馈
网络的误差反向传播算法即 BP 算法进行了详尽的分析，解决了长期以来没有权值调整有效算法的难题。  

---

## 重要影响的神经网络   

 网络名称 | 特点| 局限性 | 应用领域   
--------------------------------------| ------------------------------------------------------ | ----------------------------------------------------- | -----------------------------------------------------   
 感知器(Perceptron）| 最早的神经网络，有学习能力，只能进行线性分类 | 不能识别复杂字符，与输入模式的大小﹑平移和旋转敏感 | 文字识别﹑声音识别和学习记忆等    
 自适应线性单元(Adaline)| 学习能力较强，较早开始商业应用 | 要求输入-输出之间是线性关系 | 雷达天线控制﹑自适应回波抵消等  
小脑自动机(Cerellatron)|能调和各种指令系列，按需要缓慢地插入动作|需要复杂的控制输入|控制机器人的手臂运动|   
误差反传网络BP(Back Propagation)|多层前馈网络，采用最小均方差学习方式，是目前应用最广泛的网络|需要大量输入-输出数据，训练时间长，易陷入局部极小|语音识别﹑过程控制﹑模式识别等  
自适应共振理论 ART|可以对任意多个和任意复杂的二维模式进行自组织学习|受平移﹑旋转和尺度的影响；系统较复杂|模式识别，长于识别复杂﹑未知模式  
盒中脑 BSB 网络 (Brain State in aBox)|具有最小均方差的单层自联想网络，类似于双向联想记忆，可对片断输入补全|只能作一次性决策，无重复性共振|解释概念形成，分类和知识处理  
新认知机(Neocognition)|多层结构化字符识别网络，与输入模式的大小﹑平移和旋转无关，能识别复杂字型|需要大量加工单元和联系|手写字母识别  
自组织特征映射网络 SOM|对输入样本自组织聚类，可映射样本空间的分布|模式类型数需要事先知道|语音识别﹑机器人控制，图像处理等   
Hopfield 网络|单层自联想网络，可从缺损或有噪声输入中恢复完整信息|无学习能力，权值要预先设定|求解TSP问题，优化计算及联想记忆等  
玻尔兹曼机&nbsp; 柯西机|采用随机学习算法的网络，可训练实现全局最优|玻尔兹曼机训练时间长；柯西机在某些统计分布下产生噪声|图像﹑声纳和雷达等的模式识别
双向联想记忆网|双向联想式单层网络，有学习功能，简单易学|存储的密度低，数据必须能编码|内容寻址的联想记忆  
双向传播网CPN|一种在功能上作为统计最优化和概率密度函数分析的网络|需要大量处理单元和连接，要高度准确|神经网络计算机，图像处理和统计分析

&nbsp;   
&nbsp;  


## 参考链接   
<a href="http://www.zhihu.com/question/22553761" target="_blank">如何形象有趣的讲解人工神经网络</a>  
<a href="http://www.zhihu.com/question/27665628" target="_blank">学习人工神经网络需要哪些前导知识</a>   
<a href="http://blog.csdn.net/cinmyheart/article/details/40428203" target="_blank">把神经网络拉下神坛</a>   
<a href="http://idl.hbdlib.cn/book/00000000000000/pdfbook/018/017/178480.pdf" target="_blank">神经网络</a>  
<a href="http://work.caltech.edu/lectures.html#lectures" target="_blank">Professor Yaser Abu-Mostafa Machine Learning</a> 









<!-- 多说评论框 start -->
<div class="ds-thread" data-thread-key="ASD" data-title="ASD" ></div>
<!-- 多说评论框 end -->
<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"goaheadalvin"};
(function() {
var ds = document.createElement('script');
ds.type = 'text/javascript';ds.async = true;
ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
ds.charset = 'UTF-8';
(document.getElementsByTagName('head')[0] 
|| document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script>
<!-- 多说公共JS代码 end -->