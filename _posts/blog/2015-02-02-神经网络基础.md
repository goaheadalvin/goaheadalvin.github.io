---
layout: post
title: "神经网络基础"
excerpt: "人工神经网络的基本模型及其功能"
categories: blog
tags: [编程, 神经网络]
author: Alvin
comments: true
share: true
---
{% include _toc.html %} 

#### MeCulloch-Pitts模型   
<figure >
<img src="/postimage/AI1/mp.jpg" alt="M-P model">
</figure> 

对于这样一种多输入、单输出的基本单元可以进一步从生物化学、电生物学、数学等方面给出描述其功能的模型。从信息处理观点考察,为神经元构作了各种形式的数学模型。
>对于第j个神经元,接受多个其它神经元的输入信号xi。各突触强度以实系数 wij 表示,这 是第 i 个神经元对第 j 个神经元作用的加权值。利用某种运算把输入信号的作用结合起来,给出它们的总效果,称为“净输入”。 
   
>净输入表达式有多种类型, 其中,最简单的一种形式是线性加权求和。此作用引起神经元 j 的状态变化,而神经元 j 的输出 yj是其当前状态的函数.

利用大量神经元相互连接组成的人工神经网络,将显示出人脑的若干特征,人工神经网络也具有初步的自适应与自组织能力。在学习或训练过程中改变突触权重 wij 值,以适应周 围环境的要求。同一网络因学习方式及内容不同可具有不同的功能。人工神经网络是一个具 有学习能力的系统,可以发展知识,以至超过设计者原有的知识水平。通常,它的学习(或 训练)方式可分为两种,一种是有监督(supervised)或称有导师的学习,这时利用给定的样本 标准进行分类或模仿;另一种是无监督(unsupervised)学习或称无导师学习,这时,只规定学 习方式或某些规则,而具体的学习内容随系统所处环境(即输入信号情况)而异,系统可以自 动发现环境特征和规律性,具有更近似于人脑的功能。  

<a href="http://hahack.com/reading/ann1/#more" target="_blank">浅谈M-P模型</a>  

#### 人工神经网络的基本功能   
<figure >
<img src="/postimage/AI1/功能1.jpg" alt="功能1">
</figure>   

<figure >
<img src="/postimage/AI1/功能2.jpg" alt="功能2">
</figure>   

<figure >
<img src="/postimage/AI1/功能3.jpg" alt="功能3">
</figure>     

#### 人工神经网络的基本要素   
- 神经元功能函数  
- 神经元之间的连接形式  
- 网络的学习(训练)     

<figure >
<img src="/postimage/AI1/功能函数1.jpg" alt="功能函数1">
</figure>   

<figure >
<img src="/postimage/AI1/功能函数2.jpg" alt="功能函数2">
</figure>   

<figure >
<img src="/postimage/AI1/功能函数3.jpg" alt="功能函数3">
</figure>    

<figure >
<img src="/postimage/AI1/功能函数4.jpg" alt="功能函数4">
</figure>   

<figure >
<img src="/postimage/AI1/功能函数5.jpg" alt="功能函数5">
</figure>   

#### 神经元之间的连接形式   
前已述及,神经网络是一个复杂的互连系统,单元之间的互连模式将对网络的性质和功能产生重要影响。  
互连模式种类繁多,这里介绍一些典型的网络结构。  
 
- 前向网络(前馈网络)  
网络可以分为若干“层”,各层按信号传输先后顺序依次排列,第 i 层的神经元只接受 第(i-1)层神经元给出的信号,各神经元之间没有反馈。  
前馈型网络可用一有向无环路图表示, 如图 1-5。可以看出,输入节点并无计算功能,只是为了表征输入矢量各元素值。  
各层节点 表示具有计算功能的神经元,称为计算单元。每个计算单元可以有任意个输入,但只有一个输出,它可送到多个节点作输入。  
称输入节点层为第零层。计算单元的各节点层从下至上依 次称为第 1 至第 N 层,由此构成 N 层前向网络。(也有把输入节点层称为第 1 层,于是对 N 层网络将变为 N+1 个节点层序号。)  
第一节点层与输出节点统称为“可见层”,而其他中间层则称为隐含层(hidden layer), 这些神经元称为隐节点。BP 网络就是典型的前向网络。  
<figure >
<img src="/postimage/AI1/前向网络结构.jpg" alt="前向网络结构">
</figure>   

- 反馈网络  
典型的反馈型神经网络如图 1-6(a),每个节点都表示一个计算单元,同时接受外加输入和其它各节点的反馈输入,每个节点也都直接向外部输出。  
Hopfield 网络即属此种类型。  
在某些反馈网络中,各神经元除接受外加输入与其它各节点反馈输入之外,还包括自身反馈。   
有时,反馈型神经网络也可表示为一张完全的无向图,如图 1-6(b)。图中,每一个连接都是 双向的。这里,第 i 个神经元对于第 j 个神经元的反馈与第 j 至 i 神经元反馈之突触权重相 等,也即 wij = wji。

 <figure >
<img src="/postimage/AI1/反馈网络.jpg" alt="反馈网络">
</figure> 

#### 人工神经网络的学习(训练)  
四十年代末,D. O. Hebb 首先提出了一种神经网络学习算法,称为 Hebb 规则。以此规则为基础,出现了多种形式的学习算法。  
下面初步介绍这一规则。

- ###### Hebb 规则  

<figure >
<img src="/postimage/AI1/hebb1.jpg" alt="hebb">
</figure> 

<figure >
<img src="/postimage/AI1/hebb2.jpg" alt="hebb2">
</figure>   

<figure >
<img src="/postimage/AI1/hebb3.png" alt="hebb3">
</figure> 


- ###### 误差修正法学习算法
下面介绍另一种学习算法,称为误差修正法.  
权值的调整与网络的输出误差有关. 它包括δ学习规则、Widrow-Hoff 学习规则、感知器学习规则和误差反向传播的 BP(Back Propagation)学习规则等。  

###### 感知器学习规则  

>1958 年,美国学者 Frank Rosenblatt 首次定义了一个具有单层计算单元的神经网络结构, 取名为感知器(Perceptron)。如果包括输入层在内,应为两层。单计算节点感知器结构如图 1-7 所示。

<figure >
<img src="/postimage/AI1/感知器.png" alt="感知器">
</figure>  

<figure >
<img src="/postimage/AI1/感知器2.png" alt="感知器2">
</figure>   



<figure >
<img src="/postimage/AI1/感知器3.png" alt="感知器3">
</figure>   

可见总存在一组 w1j , w2j ,θ 满足式(1-17),也就是说,单计算节点感知器对逻辑“与”问题具有分类能力。

<figure >
<img src="/postimage/AI1/感知器4.png" alt="感知器4">
</figure>  

单层感知器只能求解线性问题,对求解非线性问题时,需要用到多层感知器,即网络应具有隐层,但对隐层神经元的学习规则尚无所知。  
就感知器学习规则来说,其权值的调整取 决于网络期望输出与实际输出之差,而对各隐层节点来说,不存在期望输出,因而该学习规则对隐层权值调整不适用。此时需要用到误差反向传播的 BP 学习规则。  

###### δ学习规则  
<figure >
<img src="/postimage/AI1/感知器5.png" alt="感知器5">
</figure>  

###### Widrow-Hoff 学习规则   
<figure >
<img src="/postimage/AI1/感知器6.png" alt="感知器6">
</figure>  
<figure >
<img src="/postimage/AI1/感知器7.png" alt="感知器7">
</figure>  
<figure >
<img src="/postimage/AI1/感知器8.png" alt="感知器8">
</figure>   
<figure >
<img src="/postimage/AI1/感知器9.png" alt="感知器9">
</figure>  
<figure >
<img src="/postimage/AI1/感知器10.png" alt="感知器10">
</figure>  

以上几种学习规则都属于监督类型的学习。可以将这类学习过程用图 1-8 表示。图中, 样本训练数据加到网络输入端,同时将相应的期望输出与网络输出相比较得到误差信号,以 此控制权重连接强度的调整,经计算至收敛后给出确定的 w 值。当样本情况发生变化时, 经学习可修正 w 值以适应新的环境。  
<figure >
<img src="/postimage/AI1/感知器11.png" alt="感知器11">
</figure> 

###### 胜者为王(Winner-Take-All)学习规则  
<figure >
<img src="/postimage/AI1/感知器12.png" alt="感知器12">
</figure>   

#### 总结
<figure >
<img src="/postimage/AI1/总结.png" alt="总结">
</figure> 
表明两者越相似,所以调整获胜神经元权值的结果是使Wm 进一步接近当前输入 X 。显然,当下次出现与 X 相象的输入模式时,上次获胜的神经元更易获胜。  
在反复的竞争学习过程中,竞争层的各神经元所对应的权向量被逐渐调整为输入样本空间的聚类中心。

&nbsp;  

#### 参考链接  
<a href="http://people.chu.edu.tw/~cclu/EngineeringMath%28III%29/Summary%20206.pdf" target="_blank">正交向量的定义和性质</a>  
<a href="http://zh.wikipedia.org/wiki/%E6%95%B0%E9%87%8F%E7%A7%AF" target="_blank">wiki:向量点乘</a>  
<a href="http://zh.wikipedia.org/wiki/%E5%90%91%E9%87%8F%E7%A7%AF" target="_blank">wiki:向量叉乘</a>  
<a href="http://student.zjzk.cn/course_ware/web-gcsx/gcsx/chapter4/chapter4.1.htm" target="_blank">特征值与特征向量</a>




&nbsp;   

<!-- 多说评论框 start -->
<div class="ds-thread" data-thread-key="ASD" data-title="ASD" ></div>
<!-- 多说评论框 end -->
<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"goaheadalvin"};
(function() {
var ds = document.createElement('script');
ds.type = 'text/javascript';ds.async = true;
ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
ds.charset = 'UTF-8';
(document.getElementsByTagName('head')[0] 
|| document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script>
<!-- 多说公共JS代码 end -->